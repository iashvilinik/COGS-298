# COGS-298 Nikoloz Iashvili: Pulsar Star Labeling

link to the google colab notebook:https://drive.google.com/file/d/1FsE-QF0L-7PWg9OU_ewIfvFuuGFX5Dcw/view?usp=sharing

This deep neural network attempts to correctly label neutron stars as pulsars based on mean, standart deviation, skewness, and excess kurtosis of two observable features of a star. The dataset can be found on kaggle: https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star. The working dataset simply moves column 9 to the first column.

Although it is possible to eventually identiy a neutron star as a pulsar, the aim is to be able to correctly label one sooner by just observing these eight features. As a personal interest, this is a project that builds a network that's applicable to various datasets, whether it's a binary classification problem or a regression problem. I also used it to attempt predicting the price of S&P500 index using the price data of all of its component stocks from a minute before. Future aims of this network are to predict the next minute price of a particular stock by using lagged values of the price. In such dataset, an important objective is to find the optimal number of lagged values to consider in the network, so that needs a bit more work than what was in the scope of the Deep Learning specialization on Coursera.

The network architecture was inspired by the S&P500 data. I would call it triangular. It has 8 hidden layers with the first one having 2048 hidden units, and each consecutive one having twice as few until the 16 units in the 8th layer, and 1 uint for binary classification in the output layer. 

For pulsar star labeling this architecture can be intuitively justified as follows: The huge number of neurons in the first layer should capture complex interactions between the eight variables, if there are any. Since these variables are essentially sample moments, various combinations of them indicate to noticeable differnces about the actual data, and given that a neutron star being a pulsar or not depends on more features than eight, we want to extract as much information from these momens as possible. 
The shrinking size of hidden layers is a version of regularization. If we expect that, given no dropout, the network would highly depend on some particular interactions anyways, this architecture allows it to focus on features more and more before the classification is made.
More on dropout implementation in a bit.

To prepare the data, I scaled the input variables in train and test sets separately. These were split 85% to 15%. Since the output variable was binary as 0 or 1, I left it alone and decided to use sigmoid activation functions in each layer. Having tried ReLu and some versions of it with stock price data, I wanted to avoid the "dying ReLu" problem which sets all weights to 0 eventually.

Each layer has a dropout feature with a value of 0.5. This was suggester by various sources and the arguement was that it allows for the largest set of possible networks, meaning possible combinations of networks with different neurons switched off. The depth and width of the network was simply chosen to be "large" enough, but according to the course material on Coursera, with regolarization in place, there seems to be no down side to a larger-then-necessary network. 

Through attempts in building the network it seemed that It's not possible, or at least not as far as I could tell, to use Tensorflow while maintaining the notation used by Andrew Ng on coursera. Specifically, he made the rows of input matrix X represent features, and columns represent training sets. Since Coursera assignments eventually got blocked for non-paying users, I could not confirm if this is possible. That's because in that way the y vector surely needs to be a 2D numpy array of shape (length, 1), but tensorflow does not accept that, and wants it to be 1D. So, it's better to leave datasets the way they usually are, which is, columns representing features. 

The results were not particularly enlightening. Strangely, at some point they seemed great, but after switching the dataset out and back in, the network behaved differently and I could not restore the original neat decline in loss from both, training and test sets. First, it seems like the network refuses to learn at all without implementing dropout. Second, even with drouput, there is a pattern in the training set loss that does not want to converge to 0, although the training set does. 

The results are displayed on the plots at the bottom of the model. The top plot traces out losses evaluated at training set and testing set in each iteration, and the bottom plot maps the output layer values on top of pulsar star columnts. Ideally, the red columns would completely cover the blue ones.

To run the model: Once the dataset, or any similar dataset is acquired, move the predictor variable to the first column. Change the 5th chunk to read in the correct .csv file. Then, input variables are scaled from 0 to one, and in this case the class labels are left alone. If it's a regression type dataset, y should be rescaled as well. The chunk with model hyperparameters can be adjusted to one's preference, however I found the current values to be a good fit. The number of neutons can be reduced down, but that gave much worse results. The rest can be run as is.

Possible areas of further investigation: First of all, not being able to use TensorBoard really put a hurdle on diagnosing the model. Since the code is in Colab, I tried to both download the saved model, which gave an error, and use a script to create a link to TensorBoard on the spot but nothing would display. This problem persisted for quite a while. Aside from that, I have tried various activation functions and their combinations as well, on this dataset and others. Sigmoid seemed to work the best, and is especially good given the data scaling from 0 to 1. In that sense it is even usable for continuous y values as long as extreme outliers are daelt with. The loss function is not cross-entropy (similar to what Andrew Ng uses), but is mean squared error. This should be fine for classification since the same principle drives it to zero: a perfect match of output and y tensors. The issue of gradient descent convergence I already investigated. First, using the Adam optimizer should take care of a lot of issues, especially since it uses momentum in descent. So, if learning rate is low enough, the momentum decreases fast enough to be very slow by the time a local minimum is reached. The idea of local minima seems unlikely since weights are initialized as random values from a uniform normal. One missing component in this model is layer normalization. As in, to normalize the size of every hidden layer. Since the losss in the model fluctuates so much, it may be the case that the model is not properly fitting to the training sets overall. Finally, since it's a classification problem, and the output is on a scale of 0 to 1, another feature to optimize is the classifier cutoff. It's not necessary for the cutoff to be 0.5. Instead, it can be maximized based on what fraction of pulsars the cutoff labels correctly, and what fraction it labels incorrectly. The correct cutoff is the one that grants the highest total correct classification.

Future prospects: If I manage to fix this network, then I want to be able to generalize it for more datasets, and put the network build into a function where activation function, network size and depth are arguements. This makes the testing much more smooth. Eventually, this model should take in lagged values of stock price data and I want to compare its performance to that of a Recurrent Neural Network. However, building one is more complicated with tensorflow. It needs the "Sequential()" class of keras package, but it's nuances have to be understood better. 
